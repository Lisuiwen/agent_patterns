import "dotenv/config";
import { Annotation, StateGraph, END } from "@langchain/langgraph";
import { HumanMessage, SystemMessage, BaseMessage, RemoveMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";

const MemoryState = Annotation.Root({
  messages: Annotation<BaseMessage[]>({ reducer: (x, y) => x.concat(y), default: () => [] }),
  summary: Annotation<string>({ reducer: (x, y) => y ?? x, default: () => "" }),
});

const CONFIG = {
  apiKey: process.env.OPENAI_API_KEY,
  configuration: { baseURL: "https://api.moonshot.cn/v1" },
  modelName: "kimi-k2-turbo-preview",
};
const model = new ChatOpenAI({ ...CONFIG, temperature: 0.5 });

async function chatNode(state: typeof MemoryState.State) {
  const { messages, summary } = state;
  let systemPrompt = "ä½ æ˜¯ä¸€ä¸ªå¥è°ˆçš„ AI æœ‹å‹ã€‚";
  if (summary) systemPrompt += `\nè¿™æ˜¯ä½ ä»¬ä¹‹å‰çš„èŠå¤©æ‘˜è¦: "${summary}"`;
  const response = await model.invoke([new SystemMessage(systemPrompt), ...messages]);
  return { messages: [response] };
}

async function summarizeNode(state: typeof MemoryState.State) {
  const { messages, summary } = state;
  console.log("\nğŸ§¹ [System] å†å²æ¶ˆæ¯è¿‡é•¿ï¼Œæ­£åœ¨è§¦å‘è®°å¿†å‹ç¼©...");
  const summaryPrompt = `è¿™æ˜¯ä¹‹å‰çš„å¯¹è¯æ‘˜è¦: "${summary}"\nè¿™æ˜¯æ–°çš„å‡ å¥å¯¹è¯:\n${messages.map(m => `${m.getType()}: ${m.content}`).join("\n")}\nè¯·ç”Ÿæˆä¸€ä¸ªæ–°çš„ã€åˆå¹¶åçš„ç®€çŸ­æ‘˜è¦ï¼Œæ¶µç›–æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚`;
  const response = await model.invoke([new HumanMessage(summaryPrompt)]);
  const newSummary = response.content as string;
  const deleteMessages = messages.slice(0, -2).map(m => new RemoveMessage({ id: m.id! }));
  console.log(`âœ… æ–°æ‘˜è¦: ${newSummary.slice(0, 30)}...`);
  return { summary: newSummary, messages: deleteMessages };
}

function shouldSummarize(state: typeof MemoryState.State) {
  return state.messages.length > 6 ? "summarize" : END;
}

const workflow = new StateGraph(MemoryState)
  .addNode("chat", chatNode)
  .addNode("summarize", summarizeNode)
  .addEdge("__start__", "chat")
  .addConditionalEdges("chat", shouldSummarize, { summarize: "summarize", [END]: END })
  .addEdge("summarize", END);

const app = workflow.compile();

async function simulate() {
  const initialHistory = [
    new HumanMessage("æˆ‘å«å°æ˜"), new BaseMessage({content: "ä½ å¥½å°æ˜", role: "assistant"}),
    new HumanMessage("å–œæ¬¢è¶³çƒ"), new BaseMessage({content: "è¶³çƒå¾ˆæ£’", role: "assistant"}),
    new HumanMessage("ä½åœ¨åŒ—äº¬"), new BaseMessage({content: "åŒ—äº¬å¾ˆå¤§", role: "assistant"}),
    new HumanMessage("æµ‹è¯•è§¦å‘"), 
  ];
  console.log("ğŸš€ æ¨¡æ‹Ÿå¸¦è®°å¿†çš„å¯¹è¯...");
  const result = await app.invoke({ messages: initialHistory });
  if (result.summary) console.log(`ğŸ‰ æˆåŠŸè§¦å‘è®°å¿†å‹ç¼©ï¼\næœ€ç»ˆæ‘˜è¦: ${result.summary}`);
}
simulate().catch(console.error);
