import "dotenv/config";
import { Annotation, StateGraph, END } from "@langchain/langgraph";
import { HumanMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";

const PlanningState = Annotation.Root({
  objective: Annotation<string>,
  plan: Annotation<string[]>({ reducer: (x, y) => y ?? x, default: () => [] }),
  pastSteps: Annotation<string[]>({ reducer: (x, y) => x.concat(y), default: () => [] }),
  response: Annotation<string>,
});

const CONFIG = {
  apiKey: process.env.OPENAI_API_KEY,
  configuration: { baseURL: "https://api.moonshot.cn/v1" },
  modelName: "kimi-k2-turbo-preview",
};
const model = new ChatOpenAI({ ...CONFIG, temperature: 0 });

async function plannerNode(state: typeof PlanningState.State) {
  const { objective } = state;
  console.log(`\nğŸ“ [Planner] æ­£åœ¨åˆ¶å®šè®¡åˆ’: "${objective}"...`);
  const prompt = `ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚\nç›®æ ‡: ${objective}\nè¯·ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ­¥éª¤æ¸…å•æ¥å®ç°è¿™ä¸ªç›®æ ‡ã€‚è¦æ±‚ï¼šæœ€å¤š 3-4 ä¸ªæ­¥éª¤ã€‚è¿”å›æ ¼å¼å¿…é¡»æ˜¯çº¯æ–‡æœ¬çš„åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªæ­¥éª¤ã€‚`;
  const response = await model.invoke([new HumanMessage(prompt)]);
  const plan = response.content.toString().split('\n').filter(line => line.trim().length > 0);
  console.log(`ğŸ“‹ è®¡åˆ’ç”Ÿæˆå®Œæ¯•ï¼Œå…± ${plan.length} æ­¥ã€‚`);
  return { plan };
}

async function executorNode(state: typeof PlanningState.State) {
  const { plan, pastSteps } = state;
  const currentStep = plan[0];
  console.log(`\nğŸ”¨ [Executor] æ­£åœ¨æ‰§è¡Œæ­¥éª¤: "${currentStep}"`);
  const context = pastSteps.map((s, i) => `æ­¥éª¤ ${i+1} ç»“æœ: ${s}`).join("\n");
  const prompt = `è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡: "${currentStep}"\n${context ? `è¿™æ˜¯ä¹‹å‰çš„æ­¥éª¤äº§ç”Ÿçš„ä¿¡æ¯(ä¾›å‚è€ƒ):\n${context}` : ""}\nè¯·ä»…è¿”å›å½“å‰ä»»åŠ¡çš„æ‰§è¡Œç»“æœã€‚`;
  const response = await model.invoke([new HumanMessage(prompt)]);
  const result = response.content as string;
  console.log(`âœ… æ­¥éª¤å®Œæˆã€‚ç»“æœé¢„è§ˆ: ${result.slice(0, 30)}...`);
  return { pastSteps: [result], plan: plan.slice(1) };
}

async function responseNode(state: typeof PlanningState.State) {
  console.log(`\nğŸ‰ [Finalizer] æ­£åœ¨æ•´åˆæœ€ç»ˆå›å¤...`);
  const { objective, pastSteps } = state;
  const prompt = `ç”¨æˆ·ç›®æ ‡: "${objective}"\næˆ‘ä»¬å·²ç»åˆ†æ­¥å®Œæˆäº†æ‰€æœ‰ä»»åŠ¡ï¼Œç»“æœå¦‚ä¸‹:\n${pastSteps.map((s, i) => `--- æ­¥éª¤ ${i+1} ---\n${s}`).join("\n")}\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç»™ç”¨æˆ·ä¸€ä¸ªè¿è´¯çš„ã€æœ€ç»ˆçš„å›å¤ã€‚`;
  const response = await model.invoke([new HumanMessage(prompt)]);
  return { response: response.content as string };
}

function shouldContinue(state: typeof PlanningState.State) {
  return state.plan.length > 0 ? "executor" : "responder";
}

const workflow = new StateGraph(PlanningState)
  .addNode("planner", plannerNode)
  .addNode("executor", executorNode)
  .addNode("responder", responseNode)
  .addEdge("__start__", "planner")
  .addEdge("planner", "executor")
  .addConditionalEdges("executor", shouldContinue, { executor: "executor", responder: "responder" })
  .addEdge("responder", END);

const app = workflow.compile();

async function main() {
  const objective = "æˆ‘æƒ³äº†è§£ Rust è¯­è¨€çš„ç‰¹ç‚¹ï¼Œå¹¶å†™ä¸€æ®µ Hello World ä»£ç è§£é‡Šå…¶è¯­æ³•";
  const result = await app.invoke({ objective });
  console.log("\n====== FINAL OUTPUT ======\n" + result.response);
}
main().catch(console.error);
